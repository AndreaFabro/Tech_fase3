{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AndreaFabro/Tech_fase3/blob/main/preparacao_dados.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Conexão com o Google Drive\n",
        "#Para utilizar o repositorio para guardar o arquivo gerado\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "ZRRzGpR0av3o",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6893bd3a-09ec-4114-8eda-257cb4c1a276"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "VvXIUylQYIEr"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "# Formatar cada linha de produto incluindo marcadores de inicio e fim de sentença\n",
        "# input: instrução, titulo e descrição\n",
        "\n",
        "def process_line_complementary_dataset(item):\n",
        "    #Processa cada linha do arquivo JSON\n",
        "      return {\n",
        "        \"input\": f\"DESCRIBE THIS PRODUCTS.\\n[|Title|] {item['title'].strip()}[|eTitle|]\\n\\n[|Content|]{item['content'].strip()}[|eContent|]\"\n",
        "        }\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "# Inicializando a lista para armazenar os dados processados\n",
        "processed_data = []\n",
        "\n",
        "# Leitura e processamento do arquivo de entrada\n",
        "# Desconsideramos os itens onde \"content\" está em branco. Com isso, os dados reduzem de 2.248.619 para 1.498.718\n",
        "# Num próximo passo, retiramos os itens onde \"title\" está em branco. Com isso, os dados reduzem de 1.498.718 para 1.390.403\n",
        "# para utilizar menos recurso de processamento\n",
        "\n",
        "input_filename = r'/content/drive/MyDrive/finetuning/trn.json'\n",
        "with open(input_filename, 'r', encoding='utf-8') as file:\n",
        "    for line in file:\n",
        "      try:\n",
        "        item = json.loads(line)\n",
        "        #Para cada item, verifica se as chaves 'title' e 'content' existem e não estão vazias ou apenas com espaços.\n",
        "        if item['title'] and item['title'].strip() and item['content'] and item['content'].strip():\n",
        "           processed_data.append(process_line_complementary_dataset(item))\n",
        "      except json.JSONDecodeError:\n",
        "            print(\"Erro ao decodificar a linha:\", line)\n",
        "\n",
        "# Exibe o número de itens processados\n",
        "print(len(processed_data))\n",
        "\n",
        "# Salvar todos os dados processados em um arquivo JSON\n",
        "output_filename = r'/content/drive/MyDrive/finetuning/titles_dataset_chat_data.json'\n",
        "with open(output_filename, 'w', encoding='utf-8') as file:\n",
        "    json.dump(processed_data, file, ensure_ascii=False, indent=4)\n",
        "\n",
        "print(f\"Todos os dados reformatados foram salvos em '{output_filename}'.\")\n"
      ],
      "metadata": {
        "id": "9dF4NkQmZqv1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "91476bbe-8c20-4c20-c0bb-a5df09eec36c"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1390403\n",
            "Todos os dados reformatados foram salvos em '/content/drive/MyDrive/finetuning/titles_dataset_chat_data.json'.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "# Função para dividir o arquivo em partes menores\n",
        "def split_json_file(input_filename, output_basename, chunk_size):\n",
        "    # Lista para armazenar registros temporariamente\n",
        "    chunk = []\n",
        "    # Contador de arquivos gerados\n",
        "    file_count = 0\n",
        "\n",
        "    with open(input_filename, 'r', encoding='utf-8') as file:\n",
        "        # Ler o arquivo JSON como uma lista de objetos (espera-se que o arquivo seja uma lista de JSONs)\n",
        "        data = json.load(file)\n",
        "        total_records = len(data)\n",
        "\n",
        "        # Iterar sobre os dados e dividir em blocos\n",
        "        for index, item in enumerate(data):\n",
        "            chunk.append(item)\n",
        "\n",
        "            # Verifica se o chunk atingiu o tamanho desejado ou se é o último registro\n",
        "            if len(chunk) == chunk_size or index == total_records - 1:\n",
        "                # Define o nome do arquivo de saída\n",
        "                output_filename = f\"{output_basename}_{file_count + 1}.json\"\n",
        "\n",
        "                # Salvar o chunk no arquivo\n",
        "                with open(output_filename, 'w', encoding='utf-8') as output_file:\n",
        "                    json.dump(chunk, output_file, ensure_ascii=False, indent=4)\n",
        "\n",
        "                print(f\"Arquivo '{output_filename}' salvo com {len(chunk)} registros.\")\n",
        "\n",
        "                # Limpa o chunk para o próximo lote\n",
        "                chunk = []\n",
        "                # Incrementa o contador de arquivos\n",
        "                file_count += 1\n",
        "\n",
        "# Parâmetros\n",
        "input_filename = r'/content/drive/MyDrive/finetuning/titles_dataset_chat_data.json'\n",
        "output_basename = r'/content/drive/MyDrive/finetuning/split_titles_dataset'\n",
        "chunk_size = 100000  # Define o número de registros por arquivo\n",
        "\n",
        "# Chama a função para dividir o arquivo\n",
        "split_json_file(input_filename, output_basename, chunk_size)"
      ],
      "metadata": {
        "id": "sVXcRGUPlQGf",
        "outputId": "36537b01-e34b-4454-c9e6-80b3781b6926",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Arquivo '/content/drive/MyDrive/finetuning/split_titles_dataset_1.json' salvo com 100000 registros.\n",
            "Arquivo '/content/drive/MyDrive/finetuning/split_titles_dataset_2.json' salvo com 100000 registros.\n",
            "Arquivo '/content/drive/MyDrive/finetuning/split_titles_dataset_3.json' salvo com 100000 registros.\n",
            "Arquivo '/content/drive/MyDrive/finetuning/split_titles_dataset_4.json' salvo com 100000 registros.\n",
            "Arquivo '/content/drive/MyDrive/finetuning/split_titles_dataset_5.json' salvo com 100000 registros.\n",
            "Arquivo '/content/drive/MyDrive/finetuning/split_titles_dataset_6.json' salvo com 100000 registros.\n",
            "Arquivo '/content/drive/MyDrive/finetuning/split_titles_dataset_7.json' salvo com 100000 registros.\n",
            "Arquivo '/content/drive/MyDrive/finetuning/split_titles_dataset_8.json' salvo com 100000 registros.\n",
            "Arquivo '/content/drive/MyDrive/finetuning/split_titles_dataset_9.json' salvo com 100000 registros.\n",
            "Arquivo '/content/drive/MyDrive/finetuning/split_titles_dataset_10.json' salvo com 100000 registros.\n",
            "Arquivo '/content/drive/MyDrive/finetuning/split_titles_dataset_11.json' salvo com 100000 registros.\n",
            "Arquivo '/content/drive/MyDrive/finetuning/split_titles_dataset_12.json' salvo com 100000 registros.\n",
            "Arquivo '/content/drive/MyDrive/finetuning/split_titles_dataset_13.json' salvo com 100000 registros.\n",
            "Arquivo '/content/drive/MyDrive/finetuning/split_titles_dataset_14.json' salvo com 90403 registros.\n"
          ]
        }
      ]
    }
  ]
}